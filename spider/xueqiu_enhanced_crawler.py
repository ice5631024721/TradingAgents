#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›ªçƒç½‘ç«™å¢å¼ºç‰ˆæ•°æ®çˆ¬è™«
æ”¯æŒä»£ç†è®¾ç½®ã€åçˆ¬æœºåˆ¶å’Œæ—¶é—´èŒƒå›´è¿‡æ»¤
"""

import asyncio
import json
import re
import random
import time
from datetime import datetime, timedelta
from urllib.parse import quote
from typing import List, Dict, Optional

try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai import LLMExtractionStrategy, JsonCssExtractionStrategy
    from crawl4ai import PruningContentFilter, BM25ContentFilter
    from crawl4ai import DefaultMarkdownGenerator
except ImportError:
    print("è¯·å…ˆå®‰è£… crawl4ai: pip install crawl4ai")
    exit(1)

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("è¯·å…ˆå®‰è£… beautifulsoup4: pip install beautifulsoup4")
    exit(1)

class XueqiuEnhancedCrawler:
    """é›ªçƒç½‘ç«™å¢å¼ºç‰ˆçˆ¬è™«"""
    
    def __init__(self, use_proxy: bool = False, proxy_list: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            proxy_list: ä»£ç†åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ ['http://proxy1:port', 'http://proxy2:port']
        """
        self.use_proxy = use_proxy
        self.proxy_list = proxy_list or []
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
    def format_url(self, query_name: str) -> str:
        """æ ¼å¼åŒ–é›ªçƒæœç´¢URL"""
        encoded_query = quote(query_name)
        return f"https://xueqiu.com/k?q={encoded_query}#/timeline"
    
    def parse_time_string(self, time_str: str) -> Optional[datetime]:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        try:
            # å¤„ç†ç›¸å¯¹æ—¶é—´
            now = datetime.now()
            
            if 'åˆ†é’Ÿå‰' in time_str:
                minutes = int(re.search(r'(\d+)åˆ†é’Ÿå‰', time_str).group(1))
                return now - timedelta(minutes=minutes)
            elif 'å°æ—¶å‰' in time_str:
                hours = int(re.search(r'(\d+)å°æ—¶å‰', time_str).group(1))
                return now - timedelta(hours=hours)
            elif 'å¤©å‰' in time_str:
                days = int(re.search(r'(\d+)å¤©å‰', time_str).group(1))
                return now - timedelta(days=days)
            elif 'æ˜¨å¤©' in time_str:
                return now - timedelta(days=1)
            elif 'å‰å¤©' in time_str:
                return now - timedelta(days=2)
            elif 'åˆšåˆš' in time_str or 'åˆšæ‰' in time_str:
                return now
            else:
                # å°è¯•è§£æå…·ä½“æ—¥æœŸæ ¼å¼
                for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%m-%d %H:%M', '%mæœˆ%dæ—¥']:
                    try:
                        return datetime.strptime(time_str, fmt)
                    except ValueError:
                        continue
                        
        except Exception as e:
            print(f"è§£ææ—¶é—´å¤±è´¥: {time_str}, é”™è¯¯: {e}")
            
        return None
    
    def format_time_to_standard(self, time_str: str) -> str:
        """å°†æ—¶é—´å­—ç¬¦ä¸²æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼ YYYY-MM-DD HH:MM:SS"""
        try:
            parsed_time = self.parse_time_string(time_str)
            if parsed_time:
                # ç¡®ä¿å¹´ä»½æ­£ç¡®ï¼ˆå¦‚æœæ˜¯æœªæ¥å¹´ä»½ï¼Œè°ƒæ•´ä¸ºå½“å‰å¹´ä»½ï¼‰
                current_year = datetime.now().year
                if parsed_time.year > current_year:
                    parsed_time = parsed_time.replace(year=current_year)
                return parsed_time.strftime('%Y-%m-%d %H:%M:%S')
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å½“å‰æ—¶é—´
                return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            print(f"æ ¼å¼åŒ–æ—¶é—´å¤±è´¥: {time_str}, é”™è¯¯: {e}")
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def is_in_time_range(self, post_time: datetime, start_time: datetime, end_time: datetime) -> bool:
        """æ£€æŸ¥å¸–å­æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…"""
        return start_time <= post_time <= end_time
    
    def is_irrelevant_content(self, content: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ºé¡µé¢æ— å…³å†…å®¹ï¼ˆå¦‚ç‰ˆæƒä¿¡æ¯ã€å¹¿å‘Šã€ç™»å½•æç¤ºç­‰ï¼‰"""
        if not content or len(content.strip()) < 10:
            return True
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰ä»·å€¼çš„æŠ•èµ„å†…å®¹å…³é”®è¯
        valuable_keywords = [
            'ç‰¹æ–¯æ‹‰', 'TSLA', 'è‚¡ç¥¨', 'æŠ•èµ„', 'äº¤ä»˜', 'è´¢æŠ¥', 'ä¸šç»©', 'åˆ†æ', 
            'çœ‹å¤š', 'çœ‹ç©º', 'ä¹°å…¥', 'å–å‡º', 'æŒæœ‰', 'æ¶¨', 'è·Œ', 'è¡Œæƒ…', 'å¸‚åœº',
            'ç¾è‚¡', 'ç›˜å‰', 'ç›˜å', 'é¢„æœŸ', 'ä¸‡è¾†', 'å­£åº¦', 'å…¨çƒ', 'åˆ†æå¸ˆ',
            'é©¬æ–¯å…‹', 'é”€é‡', 'ç«äº‰', 'æ¬§æ´²', 'é”€å”®', 'æŒ‘æˆ˜', 'ä¸‹æ»‘'
        ]
        
        # å¦‚æœåŒ…å«æœ‰ä»·å€¼çš„å…³é”®è¯ï¼Œåˆ™ä¸è¿‡æ»¤
        if any(keyword in content for keyword in valuable_keywords):
            return False
        
        # å®šä¹‰æ— å…³å†…å®¹çš„å…³é”®è¯æ¨¡å¼ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
        irrelevant_patterns = [
            # ç‰ˆæƒå’Œæ³•å¾‹ä¿¡æ¯ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
            r'^.*ç‰ˆæƒ.*é›ªçƒ.*$',
            r'^.*XUEQIU\.COM.*$',
            r'^.*äº¬ICP.*$',
            r'^.*äº¬å…¬ç½‘å®‰å¤‡.*$',
            r'^.*è¥ä¸šæ‰§ç…§.*$',
            
            # ç™»å½•å’Œå®‰å…¨æç¤ºï¼ˆå®Œæ•´åŒ¹é…ï¼‰
            r'^.*è´¦å·å®‰å…¨ç­‰çº§ä½.*$',
            r'^.*ç³»ç»Ÿæ£€æµ‹åˆ°æ‚¨çš„é›ªçƒè´¦å·.*$',
            r'^.*ç»‘å®šæ‰‹æœºå·.*$',
            r'^.*æ‰«ä¸€æ‰«.*å…³æ³¨é›ªçƒ.*$',
            
            # åŠŸèƒ½æŒ‰é’®å’Œå¯¼èˆªï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            r'^(ç™»å½•|æ³¨å†Œ|ä¸‹è½½|åˆ†äº«|ä¸¾æŠ¥|æ”¶è—|å…³æ³¨|å–æ¶ˆå…³æ³¨)$',
            r'^(å‘é€éªŒè¯ç |é‡æ–°å‘é€|ç¡®å®š|å–æ¶ˆ|å…³é—­)$',
            r'^(é»˜è®¤æ’åº|æœ€æ–°è®¨è®º|è®¨è®ºæœ€å¤š|ç»¼åˆ)$',
            r'^(è‚¡ç¥¨|ç»„åˆ|ç”¨æˆ·|æœç´¢|é¦–é¡µ)$',
            
            # è®¨è®ºé™åˆ¶æç¤º
            r'^.*æœ¬å¸–è®¨è®ºæš‚æ—¶å—é™.*$',
            r'^.*é˜²è¯ˆéª—.*$',
            
            # é•¿ä¸²çš„å›½å®¶ä»£ç ï¼ˆå¤šä¸ªè¿ç»­çš„å›½å®¶ä»£ç ï¼‰
            r'.*\+\d{1,4}.*\+\d{1,4}.*\+\d{1,4}.*',
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ— å…³å†…å®¹æ¨¡å¼
        for pattern in irrelevant_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦ç”±æ•°å­—ã€ç¬¦å·å’Œç©ºæ ¼ç»„æˆï¼ˆæ”¾å®½æ ‡å‡†ï¼‰
        clean_content = re.sub(r'[\s\d\+\-\(\)\[\]\{\}\|\\]', '', content)
        if len(clean_content) < len(content) * 0.1:  # å¦‚æœæœ‰æ„ä¹‰å­—ç¬¦å°‘äº10%ï¼ˆä»30%é™ä½åˆ°10%ï¼‰
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šé‡å¤å­—ç¬¦ï¼ˆæ”¾å®½æ ‡å‡†ï¼‰
        if len(set(content)) < len(content) * 0.05:  # å¦‚æœå”¯ä¸€å­—ç¬¦å°‘äº5%ï¼ˆä»10%é™ä½åˆ°5%ï¼‰
            return True
        
        return False
    
    def get_random_proxy(self) -> Optional[str]:
        """è·å–éšæœºä»£ç†"""
        if self.use_proxy and self.proxy_list:
            return random.choice(self.proxy_list)
        return None
    
    def get_random_user_agent(self) -> str:
        """è·å–éšæœºUser-Agent"""
        return random.choice(self.user_agents)
    
    async def crawl_with_retry(self, url: str, max_retries: int = 3) -> Optional[str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å– - ä½¿ç”¨crawl4aiæœ€ä½³å®è·µ"""
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿ
                await asyncio.sleep(random.uniform(2, 5))
                
                # é…ç½®æµè§ˆå™¨ - ä½¿ç”¨crawl4aiçš„BrowserConfig
                browser_config = BrowserConfig(
                    headless=True,
                    browser_type="chromium",
                    user_agent=self.get_random_user_agent(),
                    viewport_width=1920,
                    viewport_height=1080,
                    accept_downloads=False,
                    java_script_enabled=True,
                    ignore_https_errors=True,
                    extra_args=[
                        "--no-sandbox",
                        "--disable-dev-shm-usage",
                        "--disable-gpu",
                        "--disable-web-security",
                        "--disable-features=VizDisplayCompositor"
                    ]
                )
                
                # æ·»åŠ ä»£ç†é…ç½®
                proxy = self.get_random_proxy()
                if proxy:
                    browser_config.proxy = proxy
                    print(f"ä½¿ç”¨ä»£ç†: {proxy}")
                
                # é…ç½®çˆ¬å–è¿è¡Œå‚æ•° - ä½¿ç”¨crawl4aiçš„CrawlerRunConfig
                run_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,  # ç»•è¿‡ç¼“å­˜ç¡®ä¿è·å–æœ€æ–°å†…å®¹
                    
                    # JavaScriptæ‰§è¡Œé…ç½®
                    js_code=[
                        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€é¡µé¢å‡†å¤‡
                        """
                        console.log('ğŸš€ å¼€å§‹é›ªçƒç½‘ç«™æ™ºèƒ½çˆ¬å–...');
                        
                        // ç­‰å¾…é¡µé¢åŸºç¡€åŠ è½½
                        await new Promise(resolve => {
                            if (document.readyState === 'complete') {
                                resolve();
                            } else {
                                window.addEventListener('load', resolve);
                            }
                        });
                        
                        console.log('âœ… é¡µé¢åŸºç¡€åŠ è½½å®Œæˆ');
                        """,
                        
                        # ç¬¬äºŒæ­¥ï¼šå¤„ç†å¼¹çª—å’Œå¯¼èˆª
                        """
                        console.log('ğŸ”§ å¤„ç†å¼¹çª—å’Œé¡µé¢å¯¼èˆª...');
                        
                        // å…³é—­å¯èƒ½çš„å¼¹çª—
                        const modalSelectors = [
                            '.modal__login', '.modal__security-alert', '.modal__confirm', 
                            '.modal__alert', '.widget__download-app', '.taichi__toast',
                            '[class*="modal"]', '[class*="popup"]', '[class*="dialog"]'
                        ];
                        
                        for (const selector of modalSelectors) {
                            const modals = document.querySelectorAll(selector);
                            modals.forEach(modal => {
                                if (modal && getComputedStyle(modal).display !== 'none') {
                                    const closeBtn = modal.querySelector('.close, .no-more, [class*="close"], [aria-label*="close"]');
                                    if (closeBtn) {
                                        closeBtn.click();
                                        console.log('âŒ å…³é—­å¼¹çª—:', selector);
                                    }
                                }
                            });
                        }
                        
                        await new Promise(resolve => setTimeout(resolve, 2000));
                        
                        // ç¡®ä¿åœ¨è®¨è®ºé¡µé¢
                        const timelineTab = document.querySelector('a[href="#/timeline"], [href*="timeline"]');
                        if (timelineTab && !timelineTab.classList.contains('active')) {
                            timelineTab.click();
                            console.log('ğŸ“‹ åˆ‡æ¢åˆ°è®¨è®ºé¡µé¢');
                            await new Promise(resolve => setTimeout(resolve, 3000));
                        }
                        
                        console.log('âœ… å¼¹çª—å¤„ç†å’Œå¯¼èˆªå®Œæˆ');
                        """,
                        
                        # ç¬¬ä¸‰æ­¥ï¼šç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
                        """
                        console.log('â³ ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½...');
                        
                        // æ™ºèƒ½ç­‰å¾…å†…å®¹åŠ è½½
                        let contentLoaded = false;
                        const maxWaitTime = 30000; // 30ç§’è¶…æ—¶
                        const startTime = Date.now();
                        
                        while (!contentLoaded && (Date.now() - startTime) < maxWaitTime) {
                            // æ£€æŸ¥å¤šç§å¯èƒ½çš„å†…å®¹å®¹å™¨
                            const contentSelectors = [
                                '.profiles__timeline__bd',
                                '.search__main',
                                '.timeline',
                                '[data-v-]',
                                '.feed-item',
                                '.status-item',
                                '[class*="timeline"]',
                                '[class*="feed"]',
                                '[class*="post"]'
                            ];
                            
                            let foundContent = false;
                            let contentCount = 0;
                            
                            for (const selector of contentSelectors) {
                                const container = document.querySelector(selector);
                                if (container) {
                                    const textContent = container.textContent || '';
                                    const meaningfulText = textContent.replace(/\\s+/g, ' ').trim();
                                    
                                    // æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ„ä¹‰çš„å†…å®¹
                                    if (meaningfulText.length > 200 && 
                                        !meaningfulText.includes('é»˜è®¤æ’åºæœ€æ–°è®¨è®º') &&
                                        !meaningfulText.includes('loading') &&
                                        (meaningfulText.includes('$') || meaningfulText.includes('è‚¡ç¥¨') || meaningfulText.includes('æŠ•èµ„'))) {
                                        foundContent = true;
                                        contentCount++;
                                    }
                                    
                                    // æ£€æŸ¥å…·ä½“çš„å¸–å­å…ƒç´ 
                                    const postElements = container.querySelectorAll('div[class*="item"], div[class*="post"], div[class*="feed"], div[class*="status"]');
                                    if (postElements.length > 3) {
                                        foundContent = true;
                                        contentCount += postElements.length;
                                    }
                                }
                            }
                            
                            if (foundContent && contentCount > 5) {
                                contentLoaded = true;
                                console.log(`âœ… æ£€æµ‹åˆ°æœ‰æ•ˆå†…å®¹ï¼Œå…ƒç´ æ•°é‡: ${contentCount}`);
                                break;
                            }
                            
                            // å°è¯•è§¦å‘å†…å®¹åŠ è½½
                            window.scrollTo(0, Math.min(document.body.scrollHeight / 3, 1000));
                            await new Promise(resolve => setTimeout(resolve, 1000));
                        }
                        
                        if (!contentLoaded) {
                            console.log('âš ï¸ å†…å®¹åŠ è½½è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ...');
                        }
                        """,
                        
                        # ç¬¬å››æ­¥ï¼šæ™ºèƒ½æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
                        """
                        console.log('ğŸ“œ å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹...');
                        
                        let scrollAttempts = 0;
                        const maxScrollAttempts = 10;
                        let lastHeight = document.body.scrollHeight;
                        
                        while (scrollAttempts < maxScrollAttempts) {
                            // å¹³æ»‘æ»šåŠ¨åˆ°åº•éƒ¨
                            window.scrollTo({
                                top: document.body.scrollHeight,
                                behavior: 'smooth'
                            });
                            
                            // ç­‰å¾…æ–°å†…å®¹åŠ è½½
                            await new Promise(resolve => setTimeout(resolve, 3000));
                            
                            // æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦å¢åŠ ï¼ˆè¡¨ç¤ºæœ‰æ–°å†…å®¹åŠ è½½ï¼‰
                            const currentHeight = document.body.scrollHeight;
                            if (currentHeight > lastHeight) {
                                console.log(`ğŸ“ˆ æ£€æµ‹åˆ°æ–°å†…å®¹åŠ è½½ï¼Œé¡µé¢é«˜åº¦: ${lastHeight} -> ${currentHeight}`);
                                lastHeight = currentHeight;
                            } else {
                                // å°è¯•æŸ¥æ‰¾å¹¶ç‚¹å‡»"åŠ è½½æ›´å¤š"æŒ‰é’®
                                const loadMoreButtons = document.querySelectorAll(
                                    'button, a, div[class*="load"], div[class*="more"], [onclick*="load"], [class*="next"]'
                                );
                                
                                let clickedButton = false;
                                for (const btn of loadMoreButtons) {
                                    const text = (btn.textContent || btn.innerText || '').toLowerCase();
                                    if (text.includes('æ›´å¤š') || text.includes('åŠ è½½') || 
                                        text.includes('load') || text.includes('more') || 
                                        text.includes('next') || text.includes('ç»§ç»­')) {
                                        try {
                                            btn.click();
                                            console.log('ğŸ”„ ç‚¹å‡»åŠ è½½æ›´å¤šæŒ‰é’®:', text.substring(0, 20));
                                            clickedButton = true;
                                            await new Promise(resolve => setTimeout(resolve, 2000));
                                            break;
                                        } catch(e) {
                                            console.log('âŒ ç‚¹å‡»æŒ‰é’®å¤±è´¥:', e.message);
                                        }
                                    }
                                }
                                
                                if (!clickedButton) {
                                    console.log('ğŸ›‘ æœªæ‰¾åˆ°æ›´å¤šå†…å®¹æˆ–åŠ è½½æŒ‰é’®ï¼Œåœæ­¢æ»šåŠ¨');
                                    break;
                                }
                            }
                            
                            scrollAttempts++;
                        }
                        
                        // å›åˆ°é¡¶éƒ¨
                        window.scrollTo({ top: 0, behavior: 'smooth' });
                        await new Promise(resolve => setTimeout(resolve, 2000));
                        
                        console.log('âœ… æ™ºèƒ½æ»šåŠ¨å®Œæˆ');
                        """,
                        
                        # ç¬¬äº”æ­¥ï¼šæœ€ç»ˆé¡µé¢åˆ†æå’Œä¼˜åŒ–
                        """
                        console.log('ğŸ” æ‰§è¡Œæœ€ç»ˆé¡µé¢åˆ†æ...');
                        
                        // é¡µé¢ç»Ÿè®¡ä¿¡æ¯
                        const stats = {
                            title: document.title,
                            url: window.location.href,
                            htmlLength: document.documentElement.outerHTML.length,
                            bodyTextLength: document.body.innerText.length
                        };
                        
                        // åˆ†æä¸»è¦å†…å®¹åŒºåŸŸ
                        const mainContainers = {
                            searchMain: !!document.querySelector('.search__main'),
                            timelineBd: !!document.querySelector('.profiles__timeline__bd'),
                            timelineHd: !!document.querySelector('.profiles__timeline__hd')
                        };
                        
                        // ç»Ÿè®¡å¯èƒ½çš„å¸–å­å…ƒç´ 
                        const postElements = document.querySelectorAll(
                            '.profiles__timeline__bd > *, [class*="status"], [class*="post"], [class*="item"], [class*="card"], [class*="feed"]'
                        );
                        
                        console.log('ğŸ“Š é¡µé¢åˆ†æç»“æœ:', {
                            ...stats,
                            ...mainContainers,
                            postElementsCount: postElements.length,
                            hasStockSymbols: document.body.innerText.includes('$'),
                            hasDiscussion: document.body.innerText.includes('è®¨è®º')
                        });
                        
                        console.log('ğŸ‰ é›ªçƒç½‘ç«™çˆ¬å–å‡†å¤‡å®Œæˆï¼');
                        """
                    ],
                    
                    # ç­‰å¾…æ¡ä»¶
                    wait_for="css:body",
                    
                    # é¡µé¢åŠ è½½è¶…æ—¶
                    page_timeout=60000,  # 60ç§’
                    
                    # å»¶è¿Ÿè¿”å›HTMLï¼Œç¡®ä¿æ‰€æœ‰å¼‚æ­¥å†…å®¹åŠ è½½å®Œæˆ
                    delay_before_return_html=5.0
                )
                
                # æ‰§è¡Œçˆ¬å–
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    print(f"ğŸš€ ç¬¬ {attempt + 1} æ¬¡å°è¯•çˆ¬å–: {url}")
                    result = await crawler.arun(url=url, config=run_config)
                    
                    if result.success and result.html:
                        print(f"âœ… çˆ¬å–æˆåŠŸï¼HTMLé•¿åº¦: {len(result.html)}, Markdowné•¿åº¦: {len(result.markdown) if result.markdown else 0}")
                        
                        # ä¿å­˜è°ƒè¯•HTML
                        with open('/Users/lingxiao/PycharmProjects/TradingAgents/spider/debug_html.html', 'w', encoding='utf-8') as f:
                            f.write(result.html)
                        print("ğŸ’¾ è°ƒè¯•HTMLå·²ä¿å­˜")
                        
                        return result.html
                    else:
                        error_msg = result.error_message if hasattr(result, 'error_message') else 'æœªçŸ¥é”™è¯¯'
                        print(f"âŒ çˆ¬å–å¤±è´¥: {error_msg}")
                        
            except Exception as e:
                print(f"ğŸ’¥ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¼‚å¸¸: {str(e)}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(5, 10) * (attempt + 1)
                    print(f"â° ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                    
        return None
    
    def extract_posts_from_html(self, html: str, query_name: str) -> List[Dict]:
        """ä»HTMLä¸­æå–å¸–å­ä¿¡æ¯"""
        posts = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            print(f"HTMLè§£ææˆåŠŸï¼Œé¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            
            # ä¿å­˜è°ƒè¯•ç”¨çš„HTML
            with open('/Users/lingxiao/PycharmProjects/TradingAgents/spider/debug_html.html', 'w', encoding='utf-8') as f:
                f.write(html)
            print("è°ƒè¯•HTMLå·²ä¿å­˜åˆ° debug_html.html")
            
            # é›ªçƒç½‘ç«™ç‰¹å®šçš„é€‰æ‹©å™¨ç­–ç•¥ï¼ˆåŸºäºå®é™…é¡µé¢ç»“æ„ï¼‰
            selectors = [
                # é›ªçƒè®¨è®ºé¡µé¢çš„ä¸»è¦å†…å®¹åŒºåŸŸ
                '.profiles__timeline__bd > *',
                '.profiles__timeline__bd div',
                '.search__main div',
                # é›ªçƒç‰¹å®šçš„ç±»å
                'div[class*="timeline"]',
                'div[class*="status"]', 
                'div[class*="feed"]',
                'div[class*="item"]',
                'div[class*="card"]',
                'div[class*="post"]',
                'div[class*="content"]',
                'div[class*="message"]',
                'div[class*="comment"]',
                'div[class*="discuss"]',
                # é€šç”¨é€‰æ‹©å™¨
                'article',
                '.timeline-item',
                '.status-item', 
                '.feed-item',
                '.post-item',
                '.card-item',
                # React/Vueç»„ä»¶å¯èƒ½çš„ç±»å
                '[class*="Timeline"]',
                '[class*="Status"]',
                '[class*="Feed"]',
                '[class*="Post"]',
                '[class*="Card"]',
                '[class*="Item"]',
                '[class*="Message"]',
                '[class*="Content"]',
                # æ•°æ®å±æ€§
                '[data-type="status"]',
                '[data-type="post"]',
                '[data-type="timeline"]',
                '[data-type="item"]',
                # é€šç”¨å®¹å™¨
                'section',
                '.container > div',
                '.main > div',
                # å¯èƒ½åŒ…å«ç”¨æˆ·ç”Ÿæˆå†…å®¹çš„å…ƒç´ 
                '[class*="user"]',
                '[class*="author"]',
                '[class*="discussion"]'
            ]
            
            all_elements = []
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    all_elements.extend(elements)
            
            # å»é‡
            unique_elements = []
            seen_elements = set()
            for element in all_elements:
                element_id = id(element)
                if element_id not in seen_elements:
                    seen_elements.add(element_id)
                    unique_elements.append(element)
            
            print(f"å»é‡åæœ‰ {len(unique_elements)} ä¸ªå”¯ä¸€å…ƒç´ ")
            
            # å¦‚æœä»ç„¶æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ›´å®½æ³›çš„æœç´¢
            if not unique_elements:
                print("å°è¯•æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å…ƒç´ ...")
                
                # é¦–å…ˆå°è¯•æŸ¥æ‰¾é›ªçƒç‰¹å®šçš„ç»“æ„
                timeline_bd = soup.select_one('.profiles__timeline__bd')
                if timeline_bd:
                    print("æ‰¾åˆ°é›ªçƒè®¨è®ºå†…å®¹åŒºåŸŸ")
                    # è·å–è¯¥åŒºåŸŸä¸‹çš„æ‰€æœ‰ç›´æ¥å­å…ƒç´ 
                    direct_children = timeline_bd.find_all(recursive=False)
                    print(f"è®¨è®ºåŒºåŸŸç›´æ¥å­å…ƒç´ æ•°é‡: {len(direct_children)}")
                    unique_elements.extend(direct_children)
                    
                    # å¦‚æœç›´æ¥å­å…ƒç´ ä¸å¤Ÿï¼Œè·å–æ‰€æœ‰å­å…ƒç´ 
                    if len(unique_elements) < 5:
                        all_children = timeline_bd.find_all('div')
                        print(f"è®¨è®ºåŒºåŸŸæ‰€æœ‰divå­å…ƒç´ æ•°é‡: {len(all_children)}")
                        unique_elements.extend(all_children)
                
                # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢
                if len(unique_elements) < 5:
                    print("ä½¿ç”¨å…³é”®è¯æœç´¢...")
                    all_divs = soup.find_all('div')
                    print(f"æ€»å…±æœ‰ {len(all_divs)} ä¸ªdivå…ƒç´ ")
                    
                    for div in all_divs:
                        text = div.get_text(strip=True)
                        # æ›´å®½æ³›çš„å…³é”®è¯åŒ¹é…
                        keywords = [query_name, 'è‚¡ç¥¨', 'æŠ•èµ„', '$', 'è®¨è®º', 'åˆ†æ', 'çœ‹å¤š', 'çœ‹ç©º', 'ä¹°å…¥', 'å–å‡º', 'æŒæœ‰']
                        if any(keyword in text for keyword in keywords) and len(text) > 15:
                            unique_elements.append(div)
                            if len(unique_elements) >= 100:  # å¢åŠ æœç´¢æ•°é‡
                                break
            
            print(f"æ€»å…±æ‰¾åˆ° {len(unique_elements)} ä¸ªå¯èƒ½çš„å¸–å­é¡¹ç›®")
            
            # æŒ‰å…ƒç´ æ–‡æœ¬é•¿åº¦æ’åºï¼Œä¼˜å…ˆå¤„ç†å†…å®¹è¾ƒå¤šçš„å…ƒç´ 
            unique_elements.sort(key=lambda x: len(x.get_text(strip=True)), reverse=True)
            
            # æå–å¸–å­ä¿¡æ¯
            processed_count = 0
            for i, element in enumerate(unique_elements[:100]):  # å¢åŠ å¤„ç†æ•°é‡
                try:
                    post_data = self.extract_single_post(element, query_name)
                    if post_data:
                        posts.append(post_data)
                        processed_count += 1
                        print(f"æˆåŠŸæå–ç¬¬ {processed_count} æ¡å¸–å­ (å…ƒç´  {i+1}/{len(unique_elements[:100])})")
                        
                        # è¾“å‡ºå¸–å­é¢„è§ˆ
                        content_preview = post_data['content'][:50] + '...' if len(post_data['content']) > 50 else post_data['content']
                        print(f"  å†…å®¹é¢„è§ˆ: {content_preview}")
                        print(f"  ç”¨æˆ·: {post_data['username']}, æ—¶é—´: {post_data['time']}")
                        if post_data['stock_tags']:
                            print(f"  è‚¡ç¥¨æ ‡ç­¾: {post_data['stock_tags']}")
                except Exception as e:
                    if i < 10:  # åªåœ¨å‰10ä¸ªå…ƒç´ å¤±è´¥æ—¶è¾“å‡ºè¯¦ç»†é”™è¯¯
                        print(f"æå–ç¬¬ {i+1} ä¸ªå…ƒç´ å¤±è´¥: {e}")
                    continue
            
            print(f"æœ€ç»ˆæå–åˆ° {len(posts)} æ¡å¸–å­")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°å¸–å­ï¼Œè¾“å‡ºæ›´å¤šè°ƒè¯•ä¿¡æ¯
            if not posts:
                print("\n=== è°ƒè¯•ä¿¡æ¯ ===")
                print(f"é¡µé¢æ€»å­—ç¬¦æ•°: {len(html)}")
                print(f"é¡µé¢åŒ…å«æŸ¥è¯¢è¯ '{query_name}' çš„æ¬¡æ•°: {html.count(query_name)}")
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®å®¹å™¨
                data_containers = soup.find_all(['script', 'div'], attrs={'id': True})
                print(f"æ‰¾åˆ° {len(data_containers)} ä¸ªå¯èƒ½çš„æ•°æ®å®¹å™¨")
                
                for container in data_containers[:5]:
                    if container.name == 'script' and 'application/json' in str(container.get('type', '')):
                        print(f"å‘ç°JSONæ•°æ®è„šæœ¬: {container.get('id')}")
                    elif container.name == 'div' and any(keyword in container.get('id', '') for keyword in ['app', 'root', 'main']):
                        print(f"å‘ç°ä¸»è¦å®¹å™¨: {container.get('id')}, å­å…ƒç´ æ•°: {len(container.find_all())}")
            
        except Exception as e:
            print(f"HTMLè§£æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
        return posts
    
    def extract_json_data(self, html: str, query_name: str) -> List[Dict]:
        """å°è¯•ä»é¡µé¢çš„JSONæ•°æ®ä¸­æå–å¸–å­ä¿¡æ¯"""
        posts = []
        
        try:
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ•°æ®çš„scriptæ ‡ç­¾
            soup = BeautifulSoup(html, 'html.parser')
            script_tags = soup.find_all('script')
            
            for script in script_tags:
                script_content = script.string or ''
                
                # æŸ¥æ‰¾å¯èƒ½çš„JSONæ•°æ®æ¨¡å¼
                json_patterns = [
                    r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                    r'window\.__NUXT__\s*=\s*({.+?});',
                    r'window\.__DATA__\s*=\s*({.+?});',
                    r'window\.g_initialProps\s*=\s*({.+?});',
                    r'__INITIAL_DATA__\s*=\s*({.+?});',
                    r'window\.SNB\s*=\s*({.+?});'
                ]
                
                for pattern in json_patterns:
                    matches = re.findall(pattern, script_content, re.DOTALL)
                    for match in matches:
                        try:
                            data = json.loads(match)
                            extracted_posts = self.parse_json_for_posts(data, query_name)
                            posts.extend(extracted_posts)
                            print(f"ä»JSONæ•°æ®ä¸­æå–åˆ° {len(extracted_posts)} æ¡å¸–å­")
                        except json.JSONDecodeError:
                            continue
                        except Exception as e:
                            print(f"è§£æJSONæ•°æ®å¤±è´¥: {e}")
                            continue
            
        except Exception as e:
            print(f"æå–JSONæ•°æ®å¤±è´¥: {e}")
        
        return posts
    
    def parse_json_for_posts(self, data: dict, query_name: str) -> List[Dict]:
        """é€’å½’è§£æJSONæ•°æ®æŸ¥æ‰¾å¸–å­ä¿¡æ¯"""
        posts = []
        
        def recursive_search(obj, path=""):
            if isinstance(obj, dict):
                # æŸ¥æ‰¾å¯èƒ½çš„å¸–å­æ•°ç»„
                for key, value in obj.items():
                    if key in ['list', 'data', 'items', 'posts', 'statuses', 'timeline', 'feeds']:
                        if isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict):
                                    post = self.extract_post_from_json_item(item, query_name)
                                    if post:
                                        posts.append(post)
                    recursive_search(value, f"{path}.{key}")
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    recursive_search(item, f"{path}[{i}]")
        
        recursive_search(data)
        return posts
    
    def extract_post_from_json_item(self, item: dict, query_name: str) -> Optional[Dict]:
        """ä»JSONé¡¹ç›®ä¸­æå–å¸–å­ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾æ–‡æœ¬å†…å®¹
            text_fields = ['text', 'content', 'description', 'title', 'body', 'message']
            content = ""
            for field in text_fields:
                if field in item and isinstance(item[field], str):
                    content = item[field]
                    break
            
            if not content or len(content) < 10:
                return None
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯
            keywords = [query_name, '$', 'è‚¡ç¥¨', 'æŠ•èµ„', 'è®¨è®º', 'åˆ†æ']
            if not any(keyword in content for keyword in keywords):
                return None
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            username = "æœªçŸ¥ç”¨æˆ·"
            user_fields = ['user', 'author', 'username', 'screen_name', 'name']
            for field in user_fields:
                if field in item:
                    if isinstance(item[field], dict):
                        username = item[field].get('name', item[field].get('screen_name', 'æœªçŸ¥ç”¨æˆ·'))
                    elif isinstance(item[field], str):
                        username = item[field]
                    break
            
            # æå–æ—¶é—´ä¿¡æ¯
            post_time = "æœªçŸ¥æ—¶é—´"
            time_fields = ['created_at', 'time', 'timestamp', 'date', 'publish_time']
            for field in time_fields:
                if field in item and item[field]:
                    post_time = str(item[field])
                    break
            
            # æå–è‚¡ç¥¨æ ‡ç­¾
            stock_tags = re.findall(r'\$([^$\s]{1,10})\$', content)
            
            # æå–å¸–å­URL
            post_url = None
            url_fields = ['url', 'link', 'target_url', 'href', 'target', 'path']
            for field in url_fields:
                if field in item and isinstance(item[field], str) and item[field]:
                    url = item[field]
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    if url.startswith('/'):
                        url = f"https://xueqiu.com{url}"
                    elif not url.startswith('http'):
                        url = f"https://xueqiu.com/{url}"
                    
                    # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¸–å­é“¾æ¥
                    if self.is_valid_post_url(url):
                        post_url = url
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°URLï¼Œå°è¯•ä»IDæ„å»º
            if not post_url:
                id_fields = ['id', 'status_id', 'statusId', 'post_id', 'postId']
                for field in id_fields:
                    if field in item and str(item[field]).isdigit():
                        post_id = item[field]
                        # å°è¯•æ„å»ºçŠ¶æ€URL
                        post_url = f"https://xueqiu.com/statuses/{post_id}"
                        break
            
            # æ„å»ºå¸–å­æ•°æ®
            post_data = {
                'username': username,
                'content': content[:300],
                'time': self.format_time_to_standard(post_time),
                'stock_tags': stock_tags,
                'url': post_url if post_url else f"https://xueqiu.com/k?q={quote(query_name)}#/timeline",
                'is_demo': False,
                'source': 'json_extraction'
            }
            
            return post_data
            
        except Exception as e:
            print(f"ä»JSONé¡¹ç›®æå–å¸–å­å¤±è´¥: {e}")
            return post_data
    
    def extract_post_url(self, element) -> Optional[str]:
        """ä»å…ƒç´ ä¸­æå–å¸–å­è¯¦ç»†é¡µé¢çš„URL"""
        try:
            # é›ªçƒç½‘ç«™çš„å¸–å­é“¾æ¥æ¨¡å¼
            link_selectors = [
                'a[href*="/statuses/"]',  # é›ªçƒçŠ¶æ€é“¾æ¥
                'a[href*="/status/"]',    # é›ªçƒçŠ¶æ€é“¾æ¥å˜ä½“
                'a[href*="/u/"]',         # ç”¨æˆ·é¡µé¢é“¾æ¥
                'a[href*="/user/"]',      # ç”¨æˆ·é¡µé¢é“¾æ¥å˜ä½“
                'a[href^="/"][href*="/"]', # ç›¸å¯¹é“¾æ¥
                'a[href*="xueqiu.com"]',   # ç»å¯¹é“¾æ¥
                'a[title]',                # å¸¦æ ‡é¢˜çš„é“¾æ¥
                'a[data-url]',             # æ•°æ®URLå±æ€§
                'a'                        # æ‰€æœ‰é“¾æ¥ä½œä¸ºæœ€åå¤‡é€‰
            ]
            
            for selector in link_selectors:
                links = element.select(selector)
                for link in links:
                    href = link.get('href', '')
                    data_url = link.get('data-url', '')
                    
                    # ä¼˜å…ˆä½¿ç”¨href
                    url = href or data_url
                    if not url:
                        continue
                    
                    # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„é“¾æ¥
                    skip_patterns = [
                        '/search', '/k?q=', '/login', '/register', '/about',
                        '/help', '/privacy', '/terms', '/contact', '/api',
                        'javascript:', 'mailto:', 'tel:', '#'
                    ]
                    
                    if any(pattern in url for pattern in skip_patterns):
                        continue
                    
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    if url.startswith('/'):
                        url = f"https://xueqiu.com{url}"
                    elif not url.startswith('http'):
                        url = f"https://xueqiu.com/{url}"
                    
                    # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¸–å­é“¾æ¥
                    if self.is_valid_post_url(url):
                        return url
            
            return None
            
        except Exception as e:
            print(f"æå–å¸–å­URLå¤±è´¥: {e}")
            return None
    
    def is_valid_post_url(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¸–å­URL"""
        try:
            # é›ªçƒå¸–å­URLçš„å¸¸è§æ¨¡å¼
            valid_patterns = [
                r'/statuses/\d+',      # çŠ¶æ€ID
                r'/status/\d+',       # çŠ¶æ€IDå˜ä½“
                r'/\d+/\d+',          # ç”¨æˆ·ID/å¸–å­ID
                r'/u/\d+',            # ç”¨æˆ·é¡µé¢
                r'/user/\d+',         # ç”¨æˆ·é¡µé¢å˜ä½“
            ]
            
            for pattern in valid_patterns:
                if re.search(pattern, url):
                    return True
            
            return False
            
        except Exception:
            return False
    
    async def crawl_post_detail(self, post_url: str) -> Optional[Dict]:
        """çˆ¬å–å¸–å­è¯¦ç»†é¡µé¢çš„å®Œæ•´å†…å®¹å’Œå‡†ç¡®æ—¶é—´"""
        try:
            print(f"æ­£åœ¨çˆ¬å–å¸–å­è¯¦ç»†é¡µé¢: {post_url}")
            
            # ä½¿ç”¨ç›¸åŒçš„çˆ¬å–é…ç½®
            html = await self.crawl_with_retry(post_url)
            if not html:
                return None
            
            # è§£æè¯¦ç»†é¡µé¢å†…å®¹
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–å‡†ç¡®çš„å‘å¸ƒæ—¶é—´
            post_time = "æœªçŸ¥æ—¶é—´"
            
            # ä¼˜å…ˆä».timeå…ƒç´ æå–æ—¶é—´ï¼ˆé›ªçƒå¸–å­è¯¦ç»†é¡µé¢ç‰¹æœ‰ï¼‰
            time_elem = soup.select_one('.time')
            if time_elem:
                # æ£€æŸ¥data-created_atå±æ€§
                timestamp_str = time_elem.get('data-created_at')
                if timestamp_str and timestamp_str.isdigit():
                    try:
                        timestamp = int(timestamp_str)
                        # å¤„ç†æ¯«ç§’æ—¶é—´æˆ³
                        if timestamp > 10**12:
                            timestamp = timestamp // 1000
                        dt = datetime.fromtimestamp(timestamp)
                        post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                        print(f"âœ… ä»è¯¦ç»†é¡µé¢data-created_atæå–æ—¶é—´: {post_time}")
                    except (ValueError, OSError) as e:
                        print(f"âš ï¸ æ—¶é—´æˆ³è½¬æ¢å¤±è´¥: {e}")
                
                # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œå°è¯•datetimeå±æ€§
                if post_time == "æœªçŸ¥æ—¶é—´":
                    datetime_attr = time_elem.get('datetime')
                    if datetime_attr:
                        try:
                            if 'T' in datetime_attr:
                                dt = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                                post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                                print(f"âœ… ä»è¯¦ç»†é¡µé¢datetimeå±æ€§æå–æ—¶é—´: {post_time}")
                        except ValueError:
                            pass
                
                # æœ€åå°è¯•æ–‡æœ¬å†…å®¹
                if post_time == "æœªçŸ¥æ—¶é—´":
                    time_text = time_elem.get_text(strip=True)
                    if time_text and 'å‘å¸ƒäº' in time_text:
                        # æå–"å‘å¸ƒäº2025-07-02 21:40"ä¸­çš„æ—¶é—´
                        time_match = re.search(r'å‘å¸ƒäº(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', time_text)
                        if time_match:
                            post_time = time_match.group(1) + ':00'  # æ·»åŠ ç§’æ•°
                            print(f"âœ… ä»è¯¦ç»†é¡µé¢æ–‡æœ¬æå–æ—¶é—´: {post_time}")
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°æ—¶é—´ï¼Œä½¿ç”¨å…¶ä»–é€‰æ‹©å™¨
            if post_time == "æœªçŸ¥æ—¶é—´":
                time_selectors = [
                    '[data-created_at]', 'time', '[datetime]', '.timestamp',
                    '.post-time', '.created-at', '.publish-time', '[title*=":"]'
                ]
                
                for selector in time_selectors:
                    elem = soup.select_one(selector)
                    if elem:
                        timestamp_str = elem.get('data-created_at')
                        if timestamp_str and timestamp_str.isdigit():
                            try:
                                timestamp = int(timestamp_str)
                                if timestamp > 10**12:
                                    timestamp = timestamp // 1000
                                dt = datetime.fromtimestamp(timestamp)
                                post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                                print(f"âœ… ä»{selector}æå–æ—¶é—´: {post_time}")
                                break
                            except (ValueError, OSError):
                                pass
            
            # é›ªçƒå¸–å­è¯¦ç»†é¡µé¢çš„å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '.status-content',           # çŠ¶æ€å†…å®¹
                '.detail-content',           # è¯¦ç»†å†…å®¹
                '.post-content',             # å¸–å­å†…å®¹
                '.article-content',          # æ–‡ç« å†…å®¹
                '[class*="content"]',        # åŒ…å«contentçš„ç±»
                '[class*="text"]',           # åŒ…å«textçš„ç±»
                '[class*="desc"]',           # åŒ…å«descçš„ç±»
                '.timeline-item-content',    # æ—¶é—´çº¿é¡¹ç›®å†…å®¹
                'article',                   # æ–‡ç« æ ‡ç­¾
                'main',                      # ä¸»è¦å†…å®¹åŒºåŸŸ
                '.main-content',             # ä¸»è¦å†…å®¹
                'p',                         # æ®µè½
                'div[data-content]'          # æ•°æ®å†…å®¹å±æ€§
            ]
            
            full_content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content_text = content_elem.get_text(strip=True)
                    if len(content_text) > len(full_content):
                        full_content = content_text
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå†…å®¹ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢æå–
            if not full_content or len(full_content) < 50:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # è·å–é¡µé¢ä¸»ä½“å†…å®¹
                body = soup.find('body')
                if body:
                    full_content = body.get_text(strip=True)
            
            # æ¸…ç†å†…å®¹
            if full_content:
                # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                full_content = re.sub(r'\s+', ' ', full_content).strip()
                
                # ç§»é™¤ä¸€äº›æ— ç”¨çš„æ–‡æœ¬æ¨¡å¼
                remove_patterns = [
                    r'ç‚¹å‡»æŸ¥çœ‹æ›´å¤š.*?(?=\s|$)',
                    r'å±•å¼€å…¨æ–‡.*?(?=\s|$)',
                    r'æ”¶èµ·.*?(?=\s|$)',
                    r'\d+èµ\s*\d+è¯„è®º.*?(?=\s|$)',
                    r'åˆ†äº«.*?(?=\s|$)',
                    r'ä¸¾æŠ¥.*?(?=\s|$)',
                    r'ç™»å½•.*?(?=\s|$)',
                    r'æ³¨å†Œ.*?(?=\s|$)'
                ]
                
                for pattern in remove_patterns:
                    full_content = re.sub(pattern, '', full_content)
                
                full_content = full_content.strip()
                
                # ä½¿ç”¨ä¼˜åŒ–åçš„æ— å…³å†…å®¹è¿‡æ»¤å™¨
                if self.is_irrelevant_content(full_content):
                    print(f"  æ£€æµ‹åˆ°æ— å…³å†…å®¹ï¼Œè·³è¿‡")
                    return None
                print(f"âœ… æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(full_content)}")
            
            # è¿”å›å†…å®¹å’Œæ—¶é—´
            result = {
                'content': full_content if full_content and len(full_content) > 20 else None,
                'time': post_time
            }
            
            return result
            
        except Exception as e:
            print(f"çˆ¬å–å¸–å­è¯¦ç»†å†…å®¹å¤±è´¥: {e}")
            return None
    
    def extract_single_post(self, element, query_name: str) -> Optional[Dict]:
        """æå–å•ä¸ªå¸–å­çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯¦ç»†é¡µé¢é“¾æ¥"""
        try:
            text = element.get_text(strip=True)
            
            # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹æˆ–æ˜æ˜¾çš„å¯¼èˆªå…ƒç´ 
            if len(text) < 10 or text in ['é»˜è®¤æ’åº', 'æœ€æ–°è®¨è®º', 'è®¨è®ºæœ€å¤š', 'ç»¼åˆ', 'è®¨è®º', 'è‚¡ç¥¨', 'ç»„åˆ', 'ç”¨æˆ·']:
                return None
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯æˆ–ç›¸å…³å†…å®¹
            keywords = [query_name, '$', 'è‚¡ç¥¨', 'æŠ•èµ„', 'è®¨è®º', 'åˆ†æ', 'çœ‹å¤š', 'çœ‹ç©º', 'ä¹°å…¥', 'å–å‡º', 'æŒæœ‰', 'æ¶¨', 'è·Œ', 'è¡Œæƒ…', 'å¸‚åœº', 'è´¢æŠ¥', 'ä¸šç»©']
            if not any(keyword in text for keyword in keywords):
                return None
            
            # æå–å¸–å­è¯¦ç»†é¡µé¢é“¾æ¥
            post_url = self.extract_post_url(element)
            
            # æå–ç”¨æˆ·å - é›ªçƒç½‘ç«™ç‰¹å®šçš„é€‰æ‹©å™¨
            username = "æœªçŸ¥ç”¨æˆ·"
            username_selectors = [
                # é›ªçƒç‰¹å®šçš„ç”¨æˆ·é“¾æ¥æ¨¡å¼
                'a[href*="/u/"]', 'a[href*="/user/"]', 'a[href*="/profile/"]',
                'a[href^="/"][href*="/"]',  # é›ªçƒç”¨æˆ·é“¾æ¥é€šå¸¸æ˜¯ /æ•°å­—ID æ ¼å¼
                # é€šç”¨ç”¨æˆ·åé€‰æ‹©å™¨
                '[class*="user"]', '[class*="author"]', '[class*="name"]', 
                '.username', '.author', '.user-name', '.author-name',
                '[class*="User"]', '[class*="Author"]', '[class*="Name"]',
                '[data-user]', '[data-author]', 'span[class*="user"]',
                'div[class*="user"]'
            ]
            
            for selector in username_selectors:
                user_elem = element.select_one(selector)
                if user_elem:
                    username_text = user_elem.get_text(strip=True)
                    # éªŒè¯ç”¨æˆ·åçš„åˆç†æ€§
                    if (username_text and 
                        len(username_text) < 50 and 
                        username_text not in ['æ›´å¤š', 'åŠ è½½', 'å±•å¼€', 'æ”¶èµ·', 'åˆ†äº«', 'è¯„è®º', 'ç‚¹èµ'] and
                        not username_text.isdigit()):
                        username = username_text
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ç”¨æˆ·åï¼Œå°è¯•æ­£åˆ™æå–
            if username == "æœªçŸ¥ç”¨æˆ·":
                user_patterns = [
                    r'@([\w\u4e00-\u9fa5]{2,20})',  # @ç”¨æˆ·å
                    r'ä½œè€…[ï¼š:](\S+)',  # ä½œè€…ï¼šç”¨æˆ·å
                    r'å‘å¸ƒè€…[ï¼š:](\S+)',  # å‘å¸ƒè€…ï¼šç”¨æˆ·å
                    r'ç”¨æˆ·[ï¼š:](\S+)'   # ç”¨æˆ·ï¼šç”¨æˆ·å
                ]
                for pattern in user_patterns:
                    user_match = re.search(pattern, text)
                    if user_match:
                        username = user_match.group(1)
                        break
            
            # æå–æ—¶é—´ - ä¼˜å…ˆä½¿ç”¨é›ªçƒç½‘ç«™çš„æ—¶é—´æˆ³
            post_time = "æœªçŸ¥æ—¶é—´"
            
            # é¦–å…ˆå°è¯•ä»data-created_atå±æ€§è·å–æ—¶é—´æˆ³ï¼ˆé›ªçƒç½‘ç«™ç‰¹æœ‰ï¼‰
            time_elem_with_data = element.select_one('[data-created_at]')
            if time_elem_with_data:
                timestamp_str = time_elem_with_data.get('data-created_at')
                if timestamp_str and timestamp_str.isdigit():
                    try:
                        timestamp = int(timestamp_str)
                        # å¤„ç†æ¯«ç§’æ—¶é—´æˆ³
                        if timestamp > 10**12:
                            timestamp = timestamp // 1000
                        dt = datetime.fromtimestamp(timestamp)
                        post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                        print(f"âœ… ä»data-created_atæå–æ—¶é—´: {post_time}")
                    except (ValueError, OSError) as e:
                        print(f"âš ï¸ æ—¶é—´æˆ³è½¬æ¢å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´æˆ³ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„æ—¶é—´é€‰æ‹©å™¨
            if post_time == "æœªçŸ¥æ—¶é—´":
                time_selectors = [
                    # é›ªçƒç‰¹å®šçš„æ—¶é—´é€‰æ‹©å™¨
                    '[class*="time"]', '[class*="date"]', 'time', '[title*=":"]',
                    '.timestamp', '.post-time', '.created-at', '.publish-time',
                    '[class*="Time"]', '[class*="Date"]', '[class*="Timestamp"]',
                    '[data-time]', '[data-date]', 'span[class*="time"]',
                    'div[class*="time"]', '.time-ago', '.relative-time',
                    # å¯èƒ½åŒ…å«æ—¶é—´çš„spanæˆ–smallå…ƒç´ 
                    'span[title]', 'small[title]', '.meta', '.info'
                ]
                
                for selector in time_selectors:
                    time_elem = element.select_one(selector)
                    if time_elem:
                        # ä¼˜å…ˆæ£€æŸ¥datetimeå±æ€§
                        datetime_attr = time_elem.get('datetime')
                        if datetime_attr:
                            try:
                                # è§£æISOæ ¼å¼æ—¶é—´
                                if 'T' in datetime_attr:
                                    dt = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                                    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
                                    post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                                    print(f"âœ… ä»datetimeå±æ€§æå–æ—¶é—´: {post_time}")
                                    break
                            except ValueError:
                                pass
                        
                        # ç„¶åæ£€æŸ¥titleå±æ€§
                        title_attr = time_elem.get('title')
                        if title_attr and len(title_attr) < 50:
                            post_time = title_attr
                            print(f"âœ… ä»titleå±æ€§æå–æ—¶é—´: {post_time}")
                            break
                        
                        # æœ€åæ£€æŸ¥æ–‡æœ¬å†…å®¹
                        time_text = time_elem.get_text(strip=True)
                        if time_text and len(time_text) < 50:
                            post_time = time_text
                            print(f"âœ… ä»æ–‡æœ¬å†…å®¹æå–æ—¶é—´: {post_time}")
                            break
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ—¶é—´ï¼Œå°è¯•æ­£åˆ™æå–
            if post_time == "æœªçŸ¥æ—¶é—´":
                time_patterns = [
                    r'(\d+åˆ†é’Ÿå‰)',
                    r'(\d+å°æ—¶å‰)', 
                    r'(\d+å¤©å‰)',
                    r'(\d+å‘¨å‰)',
                    r'(\d+ä¸ªæœˆå‰)',
                    r'(æ˜¨å¤©|å‰å¤©)',
                    r'(\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2})',
                    r'(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2})',
                    r'(\d{1,2}æœˆ\d{1,2}æ—¥\s+\d{1,2}:\d{2})',
                    r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    r'(ä»Šå¤©\s+\d{1,2}:\d{2})',
                    r'(åˆšåˆš|åˆšæ‰)'
                ]
                for pattern in time_patterns:
                    time_match = re.search(pattern, text)
                    if time_match:
                        post_time = time_match.group(1)
                        break
            
            # æå–è‚¡ç¥¨æ ‡ç­¾ - ä½¿ç”¨æ›´å…¨é¢çš„æ¨¡å¼
            stock_tags = []
            stock_patterns = [
                r'\$([^$\s]{1,10})\$',  # $è‚¡ç¥¨ä»£ç $
                r'\$([A-Z]{1,5})\b',    # $å¤§å†™å­—æ¯ä»£ç 
                r'([A-Z]{2,5})\.',      # è‚¡ç¥¨ä»£ç .
                r'#([^#\s]{1,10})#'     # #è‚¡ç¥¨æ ‡ç­¾#
            ]
            
            for pattern in stock_patterns:
                matches = re.findall(pattern, text)
                stock_tags.extend(matches)
            
            # å»é‡è‚¡ç¥¨æ ‡ç­¾
            stock_tags = list(set(stock_tags))
            
            # æå–ä¸»è¦å†…å®¹ - å°è¯•æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æœ¬
            content_selectors = [
                '[class*="content"]', '[class*="text"]', '[class*="desc"]',
                '.post-content', '.message-content', '.status-content',
                '[class*="Content"]', '[class*="Text"]', '[class*="Message"]',
                'p', '.description', '.summary'
            ]
            
            main_content = text
            for selector in content_selectors:
                content_elem = element.select_one(selector)
                if content_elem:
                    content_text = content_elem.get_text(strip=True)
                    if len(content_text) > 20:  # ç¡®ä¿å†…å®¹æœ‰æ„ä¹‰
                        main_content = content_text
                        break
            
            # æ¸…ç†å†…å®¹
            content = main_content[:300]  # å¢åŠ é•¿åº¦é™åˆ¶
            content = re.sub(r'\s+', ' ', content).strip()
            
            # ç§»é™¤ä¸€äº›æ— ç”¨çš„æ–‡æœ¬
            remove_patterns = [
                r'ç‚¹å‡»æŸ¥çœ‹æ›´å¤š.*',
                r'å±•å¼€å…¨æ–‡.*',
                r'æ”¶èµ·.*',
                r'\d+èµ\s*\d+è¯„è®º.*',
                r'åˆ†äº«.*',
                r'ä¸¾æŠ¥.*'
            ]
            
            for pattern in remove_patterns:
                content = re.sub(pattern, '', content)
            
            content = content.strip()
            
            # ç¡®ä¿å†…å®¹æœ‰æ„ä¹‰
            if len(content) < 5:
                return None
            
            # è¿‡æ»¤æ‰é¡µé¢æ— å…³å†…å®¹
            if self.is_irrelevant_content(content):
                return None
            
            # æ„å»ºå¸–å­æ•°æ®
            post_data = {
                'username': username,
                'content': content,
                'time': self.format_time_to_standard(post_time),
                'stock_tags': stock_tags,
                'url': post_url if post_url else f"https://xueqiu.com/k?q={quote(query_name)}#/timeline",
                'is_demo': False,
                'element_class': element.get('class', []) if hasattr(element, 'get') else [],
                'element_id': element.get('id', '') if hasattr(element, 'get') else ''
            }
            
            return post_data
            
        except Exception as e:
            print(f"æå–å¸–å­ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def crawl_xueqiu_data(self, query_name: str, start_time: datetime, end_time: datetime) -> List[Dict]:
        """çˆ¬å–é›ªçƒæ•°æ®çš„ä¸»å‡½æ•°"""
        print(f"å¼€å§‹çˆ¬å–é›ªçƒç½‘ç«™å…³äº '{query_name}' çš„è®¨è®ºæ•°æ®...")
        
        url = self.format_url(query_name)
        print(f"å¼€å§‹çˆ¬å–: {url}")
        print(f"æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä½¿ç”¨crawl_with_retryæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å·²åŒ…å«çˆ¬è™«é…ç½®
        
        # çˆ¬å–HTML
        html = await self.crawl_with_retry(url)
        if not html:
            print("æ— æ³•è·å–é¡µé¢å†…å®¹")
            return []
        
        print(f"è·å–åˆ°HTMLå†…å®¹ï¼Œé•¿åº¦: {len(html)} å­—ç¬¦")
        
        # ä¿å­˜è°ƒè¯•HTML
        with open('/Users/lingxiao/PycharmProjects/TradingAgents/spider/debug_html.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print("è°ƒè¯•HTMLå·²ä¿å­˜åˆ° debug_html.html")
        
        # å¤šç§æ–¹å¼æå–å¸–å­
        all_posts = []
        
        # 1. ä»HTML DOMç»“æ„æå–
        print("\n=== å°è¯•ä»HTML DOMç»“æ„æå–å¸–å­ ===")
        html_posts = self.extract_posts_from_html(html, query_name)
        all_posts.extend(html_posts)
        print(f"ä»HTML DOMæå–åˆ° {len(html_posts)} æ¡å¸–å­")
        
        # 2. ä»JSONæ•°æ®æå–
        print("\n=== å°è¯•ä»JSONæ•°æ®æå–å¸–å­ ===")
        json_posts = self.extract_json_data(html, query_name)
        all_posts.extend(json_posts)
        print(f"ä»JSONæ•°æ®æå–åˆ° {len(json_posts)} æ¡å¸–å­")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•å¸–å­ï¼Œå°è¯•æ›´å®½æ³›çš„æœç´¢
        if not all_posts:
            print("\n=== æœªæ‰¾åˆ°å¸–å­ï¼Œå°è¯•æ›´å®½æ³›çš„æœç´¢ ===")
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ–‡æœ¬çš„å…ƒç´ 
            text_elements = soup.find_all(['p', 'div', 'span', 'article', 'section'])
            print(f"æ‰¾åˆ° {len(text_elements)} ä¸ªå¯èƒ½åŒ…å«æ–‡æœ¬çš„å…ƒç´ ")
            
            for element in text_elements:
                text = element.get_text(strip=True)
                if len(text) > 30 and any(keyword in text for keyword in [query_name, 'è‚¡ç¥¨', 'æŠ•èµ„', '$']):
                    post_data = {
                        'username': "æœªçŸ¥ç”¨æˆ·",
                        'content': text[:300],  # é™åˆ¶é•¿åº¦
                        'time': "æœªçŸ¥æ—¶é—´",
                        'stock_tags': re.findall(r'\$([^$\s]{1,10})\$', text),
                        'url': url,
                        'is_demo': False,
                        'element_type': element.name
                    }
                    all_posts.append(post_data)
                    if len(all_posts) >= 20:  # é™åˆ¶æ•°é‡
                        break
            
            print(f"å®½æ³›æœç´¢æ‰¾åˆ° {len(all_posts)} æ¡å¯èƒ½çš„å¸–å­")
        
        # å»é‡ï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼åº¦ï¼‰
        unique_posts = self.deduplicate_posts(all_posts)
        print(f"å»é‡åå…±æœ‰ {len(unique_posts)} æ¡å”¯ä¸€å¸–å­")
        
        # æ·±åº¦çˆ¬å–ï¼šè·å–æ¯ä¸ªå¸–å­çš„å®Œæ•´å†…å®¹
        print("\n=== å¼€å§‹æ·±åº¦çˆ¬å–å¸–å­è¯¦ç»†å†…å®¹ ===")
        enhanced_posts = []
        # å¤„ç†æ‰€æœ‰å¸–å­ä»¥è·å–å‡†ç¡®çš„æ—¶é—´ä¿¡æ¯
        limited_posts = unique_posts[:10]  # å¤„ç†å‰10æ¡å¸–å­
        print(f"å¤„ç†å‰ {len(limited_posts)} æ¡å¸–å­ä»¥è·å–å‡†ç¡®æ—¶é—´ä¿¡æ¯")
        
        for i, post in enumerate(limited_posts):
            try:
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(limited_posts)} æ¡å¸–å­...")
                
                # å¦‚æœæœ‰è¯¦ç»†é¡µé¢é“¾æ¥ï¼Œçˆ¬å–å®Œæ•´å†…å®¹å’Œå‡†ç¡®æ—¶é—´
                # é›ªçƒURLæ ¼å¼ï¼šhttps://xueqiu.com/ç”¨æˆ·ID/å¸–å­ID
                post_url = post.get('url', '')
                print(f"  æ£€æŸ¥URL: {post_url}")
                
                url_has_xueqiu = 'xueqiu.com' in post_url
                url_has_statuses = '/statuses/' in post_url
                url_matches_pattern = bool(re.match(r'https://xueqiu\.com/\d+/\d+', post_url))
                
                print(f"  URLæ£€æŸ¥ç»“æœ: xueqiu={url_has_xueqiu}, statuses={url_has_statuses}, pattern={url_matches_pattern}")
                
                if (post_url and url_has_xueqiu and (url_has_statuses or url_matches_pattern)):
                    print(f"  âœ… URLåŒ¹é…æˆåŠŸï¼Œå¼€å§‹çˆ¬å–è¯¦ç»†é¡µé¢: {post_url}")
                    detail_result = await self.crawl_post_detail(post_url)
                    if detail_result and detail_result.get('content'):
                        full_content = detail_result['content']
                        detail_time = detail_result['time']
                        
                        if len(full_content) > len(post['content']):
                            print(f"  æˆåŠŸè·å–å®Œæ•´å†…å®¹ï¼Œé•¿åº¦ä» {len(post['content'])} å¢åŠ åˆ° {len(full_content)}")
                            post['content'] = full_content
                        
                        # æ›´æ–°æ—¶é—´ä¿¡æ¯ï¼ˆå¦‚æœä»è¯¦ç»†é¡µé¢è·å–åˆ°äº†å‡†ç¡®æ—¶é—´ï¼‰
                        if detail_time != "æœªçŸ¥æ—¶é—´":
                            print(f"  æ›´æ–°æ—¶é—´ä¿¡æ¯ï¼šä» {post['time']} æ›´æ–°ä¸º {detail_time}")
                            post['time'] = detail_time
                        
                        post['content_enhanced'] = True
                    else:
                        print(f"  æœªèƒ½è·å–åˆ°æ›´å®Œæ•´çš„å†…å®¹ï¼Œä¿æŒåŸå†…å®¹")
                        post['content_enhanced'] = False
                else:
                    print(f"  âŒ URLä¸åŒ¹é…ï¼Œè·³è¿‡è¯¦ç»†é¡µé¢çˆ¬å–")
                    post['content_enhanced'] = False
                
                enhanced_posts.append(post)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(limited_posts) - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(random.uniform(1, 2))  # å‡å°‘å»¶è¿Ÿæ—¶é—´
                    
            except Exception as e:
                print(f"  å¤„ç†ç¬¬ {i+1} æ¡å¸–å­æ—¶å‡ºé”™: {e}")
                enhanced_posts.append(post)  # å³ä½¿å‡ºé”™ä¹Ÿä¿ç•™åŸå§‹å¸–å­
                continue
        
        # å°†æœªå¤„ç†çš„å¸–å­ä¹ŸåŠ å…¥ç»“æœï¼ˆä¸è¿›è¡Œæ·±åº¦çˆ¬å–ï¼‰
        for post in unique_posts[10:]:
            post['content_enhanced'] = False
            enhanced_posts.append(post)
        
        print(f"æ·±åº¦çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {len(enhanced_posts)} æ¡å¸–å­ï¼Œå…¶ä¸­ {len(limited_posts)} æ¡è¿›è¡Œäº†æ·±åº¦çˆ¬å–")
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´
        filtered_posts = []
        for post in enhanced_posts:
            post_datetime = self.parse_time_string(post['time'])
            if post_datetime and self.is_in_time_range(post_datetime, start_time, end_time):
                filtered_posts.append(post)
            elif post['time'] == "æœªçŸ¥æ—¶é—´":  # å¦‚æœæ—¶é—´æœªçŸ¥ï¼Œä¹ŸåŒ…å«è¿›æ¥
                filtered_posts.append(post)
        
        print(f"\n=== æœ€ç»ˆç»“æœ ===")
        print(f"å…±çˆ¬å–åˆ° {len(unique_posts)} æ¡å¸–å­ï¼Œæ—¶é—´èŒƒå›´å†…æœ‰ {len(filtered_posts)} æ¡")
        
        # å¦‚æœç»“æœä¸ºç©ºï¼Œæ·»åŠ ä¸€ä¸ªè°ƒè¯•ä¿¡æ¯å¸–å­
        if not filtered_posts:
            debug_post = {
                'username': "è°ƒè¯•ä¿¡æ¯",
                'content': f"æœªèƒ½ä»é›ªçƒç½‘ç«™æå–åˆ°å…³äº '{query_name}' çš„å¸–å­ã€‚è¯·æ£€æŸ¥ç½‘ç«™ç»“æ„æˆ–è°ƒæ•´çˆ¬è™«å‚æ•°ã€‚",
                'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'stock_tags': [],
                'url': url,
                'is_demo': False
            }
            filtered_posts.append(debug_post)
        
        return filtered_posts
    
    def deduplicate_posts(self, posts: List[Dict]) -> List[Dict]:
        """å»é™¤é‡å¤çš„å¸–å­"""
        unique_posts = []
        seen_contents = set()
        
        for post in posts:
            # ä½¿ç”¨å†…å®¹çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå»é‡æ ‡å‡†
            content_key = post['content'][:100].strip()
            if content_key not in seen_contents and len(content_key) > 10:
                seen_contents.add(content_key)
                unique_posts.append(post)
        
        return unique_posts
    
    def save_results(self, posts: List[Dict], filename: str = None) -> str:
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'/Users/lingxiao/PycharmProjects/TradingAgents/spider/xueqiu_data_{timestamp}.json'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename

# ç¤ºä¾‹ä»£ç†åˆ—è¡¨ï¼ˆéœ€è¦æ›¿æ¢ä¸ºçœŸå®å¯ç”¨çš„ä»£ç†ï¼‰
SAMPLE_PROXIES = [
    # 'http://proxy1.example.com:8080',
    # 'http://proxy2.example.com:8080',
    # 'socks5://proxy3.example.com:1080'
]

async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = XueqiuEnhancedCrawler(
        use_proxy=False,  # è®¾ç½®ä¸ºTrueå¯ç”¨ä»£ç†
        proxy_list=SAMPLE_PROXIES
    )
    
    # è®¾ç½®çˆ¬å–å‚æ•°ï¼ˆç¼©çŸ­æ—¶é—´èŒƒå›´ä»¥åŠ å¿«æµ‹è¯•ï¼‰
    query_name = "ç‰¹æ–¯æ‹‰"  # æŸ¥è¯¢åç§°
    start_time = datetime.now() - timedelta(days=1)  # å¼€å§‹æ—¶é—´ï¼ˆ1å¤©å‰ï¼Œç¼©çŸ­æµ‹è¯•æ—¶é—´ï¼‰
    end_time = datetime.now()  # ç»“æŸæ—¶é—´ï¼ˆç°åœ¨ï¼‰
    
    try:
        # æ‰§è¡Œçˆ¬å–
        posts = await crawler.crawl_xueqiu_data(query_name, start_time, end_time)
        
        if posts:
            # ä¿å­˜ç»“æœ
            filename = crawler.save_results(posts)
            
            # è¾“å‡ºç»“æœ
            print("\n<results>")
            for i, post in enumerate(posts, 1):
                print(f"\nå¸–å­ {i}:")
                print(f"ç”¨æˆ·: {post['username']}")
                print(f"æ—¶é—´: {post['time']}")
                print(f"å†…å®¹: {post['content']}")
                if post['stock_tags']:
                    print(f"è‚¡ç¥¨æ ‡ç­¾: {', '.join(['$' + tag + '$' for tag in post['stock_tags']])})")
                print(f"é“¾æ¥: {post['url']}")
            print("\n</results>")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nç»Ÿè®¡ä¿¡æ¯:")
            print(f"- æ€»å¸–å­æ•°: {len(posts)}")
            print(f"- åŒ…å«è‚¡ç¥¨æ ‡ç­¾çš„å¸–å­: {sum(1 for p in posts if p['stock_tags'])}")
            print(f"- å¹³å‡å†…å®¹é•¿åº¦: {sum(len(p['content']) for p in posts) // len(posts) if posts else 0}å­—ç¬¦")
            
        else:
            print("æœªçˆ¬å–åˆ°ä»»ä½•æ•°æ®")
            
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())